import pandas as pd
import numpy as np
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import re
from typing import Dict, List, Tuple
from datetime import datetime, timedelta
import json

class YouTubeContentAnalyzer:
    """YouTube ì½˜í…ì¸  ê¹Šì´ ë¶„ì„ ë° ì‚¬ì—…ì„± í‰ê°€"""
    
    def __init__(self):
        self.categories = {
            1: 'Film & Animation', 2: 'Autos & Vehicles', 10: 'Music',
            15: 'Pets & Animals', 17: 'Sports', 19: 'Travel & Events',
            20: 'Gaming', 22: 'People & Blogs', 23: 'Comedy',
            24: 'Entertainment', 25: 'News & Politics', 26: 'Howto & Style',
            27: 'Education', 28: 'Science & Technology', 29: 'Nonprofits & Activism'
        }
        
    def analyze_content_performance(self, videos: List[Dict]) -> Dict:
        """ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„"""
        if not videos:
            return {}
            
        df = pd.DataFrame(videos)
        
        # ê¸°ë³¸ í†µê³„
        stats = {
            'total_videos': len(videos),
            'total_views': df['view_count'].sum(),
            'avg_views': df['view_count'].mean(),
            'median_views': df['view_count'].median(),
            'max_views': df['view_count'].max(),
            'min_views': df['view_count'].min(),
            'std_views': df['view_count'].std(),
            'avg_duration': df['duration'].mean() / 60,  # ë¶„ ë‹¨ìœ„
            'total_watch_time': df['view_count'].sum() * df['duration'].mean() / 3600  # ì‹œê°„ ë‹¨ìœ„
        }
        
        # ì°¸ì—¬ìœ¨ ë¶„ì„
        df['engagement_rate'] = ((df['like_count'] + df['comment_count']) / df['view_count'] * 100)
        stats['avg_engagement_rate'] = df['engagement_rate'].mean()
        
        # ì„±ê³¼ êµ¬ê°„ë³„ ë¶„ì„
        df['performance_tier'] = pd.cut(df['view_count'], 
                                       bins=[-1, df['view_count'].quantile(0.25), 
                                            df['view_count'].quantile(0.75), float('inf')],
                                       labels=['Low', 'Medium', 'High'])
        
        tier_analysis = {}
        for tier in ['Low', 'Medium', 'High']:
            tier_data = df[df['performance_tier'] == tier]
            if not tier_data.empty:
                tier_analysis[tier] = {
                    'count': len(tier_data),
                    'avg_views': tier_data['view_count'].mean(),
                    'avg_engagement': tier_data['engagement_rate'].mean(),
                    'avg_duration': tier_data['duration'].mean() / 60
                }
        
        stats['performance_tiers'] = tier_analysis
        
        return stats
    
    def analyze_title_patterns(self, videos: List[Dict]) -> Dict:
        """ì œëª© íŒ¨í„´ ë° í‚¤ì›Œë“œ ë¶„ì„"""
        if not videos:
            return {}
            
        titles = [video['title'] for video in videos]
        
        # ì œëª© ê¸¸ì´ ë¶„ì„
        title_lengths = [len(title) for title in titles]
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        all_text = ' '.join(titles)
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', all_text)
        english_words = re.findall(r'[a-zA-Z]+', all_text.lower())
        numbers = re.findall(r'\d+', all_text)
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ë¶„ì„
        patterns = {
            'question_marks': len(re.findall(r'\?', all_text)),
            'exclamation_marks': len(re.findall(r'!', all_text)),
            'brackets': len(re.findall(r'[\[\](){}]', all_text)),
            'numbers_in_titles': len([title for title in titles if re.search(r'\d', title)]),
            'caps_titles': len([title for title in titles if any(c.isupper() for c in title)])
        }
        
        # ê°ì • ë¶„ì„ (ì˜ì–´ ì œëª©ì— ëŒ€í•´)
        english_titles = [title for title in titles if re.search(r'[a-zA-Z]', title)]
        sentiments = []
        for title in english_titles:
            try:
                sentiment = TextBlob(title).sentiment.polarity
                sentiments.append(sentiment)
            except:
                sentiments.append(0)
        
        return {
            'avg_title_length': np.mean(title_lengths),
            'title_length_range': [min(title_lengths), max(title_lengths)],
            'top_korean_words': Counter(korean_words).most_common(10),
            'top_english_words': Counter(english_words).most_common(10),
            'common_numbers': Counter(numbers).most_common(5),
            'special_patterns': patterns,
            'avg_sentiment': np.mean(sentiments) if sentiments else 0,
            'positive_titles_ratio': len([s for s in sentiments if s > 0.1]) / len(sentiments) if sentiments else 0
        }
    
    def analyze_upload_patterns(self, videos: List[Dict]) -> Dict:
        """ì—…ë¡œë“œ íŒ¨í„´ ë¶„ì„"""
        if not videos:
            return {}
            
        df = pd.DataFrame(videos)
        df['published_date'] = pd.to_datetime(df['published_date'])
        
        # ì‹œê°„ë³„ ë¶„ì„
        df['hour'] = df['published_date'].dt.hour
        df['day_of_week'] = df['published_date'].dt.day_name()
        df['month'] = df['published_date'].dt.month
        df['day_of_month'] = df['published_date'].dt.day
        
        # ì—…ë¡œë“œ ë¹ˆë„ ê³„ì‚°
        date_range = (df['published_date'].max() - df['published_date'].min()).days
        upload_frequency = len(videos) / max(date_range, 1)
        
        # ì„±ê³¼ê°€ ì¢‹ì€ ì—…ë¡œë“œ ì‹œê°„ ë¶„ì„
        hourly_performance = df.groupby('hour').agg({
            'view_count': ['mean', 'count'],
            'like_count': 'mean',
            'comment_count': 'mean'
        }).round(0)
        
        daily_performance = df.groupby('day_of_week').agg({
            'view_count': ['mean', 'count'],
            'like_count': 'mean'
        }).round(0)
        
        # ì—…ë¡œë“œ ì¼ê´€ì„± ë¶„ì„
        upload_dates = df['published_date'].dt.date
        date_diffs = [(upload_dates.iloc[i] - upload_dates.iloc[i+1]).days 
                     for i in range(len(upload_dates)-1)]
        consistency_score = 100 - (np.std(date_diffs) / np.mean(date_diffs) * 50) if date_diffs else 0
        
        return {
            'total_days_analyzed': date_range,
            'videos_per_day': upload_frequency,
            'upload_consistency_score': max(0, consistency_score),
            'best_upload_hours': hourly_performance.nlargest(3, ('view_count', 'mean')),
            'best_upload_days': daily_performance.nlargest(3, ('view_count', 'mean')),
            'upload_distribution': {
                'hourly': df['hour'].value_counts().to_dict(),
                'daily': df['day_of_week'].value_counts().to_dict(),
                'monthly': df['month'].value_counts().to_dict()
            }
        }
    
    def analyze_content_topics(self, videos: List[Dict]) -> Dict:
        """ì½˜í…ì¸  ì£¼ì œ ë¶„ì„"""
        if not videos:
            return {}
        
        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_content = []
        for video in videos:
            content = f"{video.get('title', '')} {video.get('description', '')}"
            all_content.append(content)
        
        # ì£¼ì œë³„ í‚¤ì›Œë“œ ë¶„ë¥˜
        topic_keywords = {
            'êµìœ¡/íŠœí† ë¦¬ì–¼': ['ë°°ìš°', 'ë°©ë²•', 'ê°•ì˜', 'ì„¤ëª…', 'ê°€ì´ë“œ', 'how', 'tutorial', 'learn', 'ì•Œë ¤', 'ê³µë¶€'],
            'ë¦¬ë·°/ë¶„ì„': ['ë¦¬ë·°', 'ë¶„ì„', 'ë¹„êµ', 'í…ŒìŠ¤íŠ¸', 'í›„ê¸°', 'review', 'test', 'í‰ê°€', 'ì²´í—˜'],
            'ì—”í„°í…Œì¸ë¨¼íŠ¸': ['ì›ƒê¸´', 'ì¬ë°ŒëŠ”', 'ë†€ë¼ìš´', 'ì‹ ê¸°í•œ', 'funny', 'amazing', 'ëª°ì¹´', 'ê°œê·¸'],
            'ë‰´ìŠ¤/ì •ë³´': ['ë‰´ìŠ¤', 'ì†Œì‹', 'ì •ë³´', 'ë°œí‘œ', 'ì¶œì‹œ', 'news', 'ì—…ë°ì´íŠ¸', 'ê³µì§€'],
            'VLOG/ì¼ìƒ': ['ì¼ìƒ', 'ë¸Œì´ë¡œê·¸', 'vlog', 'í•˜ë£¨', 'ë°ì¼ë¦¬', 'daily', 'ìƒí™œ'],
            'ê²Œì„': ['ê²Œì„', 'í”Œë ˆì´', 'ê³µëµ', 'game', 'play', 'ê²œ', 'ìŠ¤íŠ¸ë¦¬ë°'],
            'ê¸°ìˆ /IT': ['ê¸°ìˆ ', 'í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'ê°œë°œ', 'tech', 'code', 'ì•±', 'ì†Œí”„íŠ¸ì›¨ì–´']
        }
        
        topic_scores = {}
        for topic, keywords in topic_keywords.items():
            score = 0
            for content in all_content:
                content_lower = content.lower()
                for keyword in keywords:
                    score += content_lower.count(keyword.lower())
            topic_scores[topic] = score
        
        # ì£¼ì œë³„ ì„±ê³¼ ë¶„ì„
        topic_performance = {}
        for video in videos:
            content = f"{video.get('title', '')} {video.get('description', '')}".lower()
            best_topic = None
            max_score = 0
            
            for topic, keywords in topic_keywords.items():
                score = sum(content.count(keyword.lower()) for keyword in keywords)
                if score > max_score:
                    max_score = score
                    best_topic = topic
            
            if best_topic and max_score > 0:
                if best_topic not in topic_performance:
                    topic_performance[best_topic] = []
                topic_performance[best_topic].append(video['view_count'])
        
        # ì£¼ì œë³„ í‰ê·  ì„±ê³¼ ê³„ì‚°
        topic_avg_views = {}
        for topic, views in topic_performance.items():
            topic_avg_views[topic] = np.mean(views)
        
        return {
            'topic_distribution': topic_scores,
            'most_common_topic': max(topic_scores, key=topic_scores.get),
            'topic_performance': topic_avg_views,
            'best_performing_topic': max(topic_avg_views, key=topic_avg_views.get) if topic_avg_views else None
        }
    
    def calculate_business_metrics(self, channel_data: Dict, content_stats: Dict) -> Dict:
        """ì‚¬ì—…ì„± ì§€í‘œ ê³„ì‚°"""
        subscribers = channel_data.get('subscriber_count', 0)
        avg_views = content_stats.get('avg_views', 0)
        total_views = content_stats.get('total_views', 0)
        avg_engagement = content_stats.get('avg_engagement_rate', 0)
        
        # CPM ê¸°ë°˜ ìˆ˜ìµ ê³„ì‚° (í•œêµ­ ê¸°ì¤€)
        base_cpm = 2000  # ê¸°ë³¸ CPM (ì›)
        
        # ì¹´í…Œê³ ë¦¬ë³„ CPM ì¡°ì •
        category_multiplier = {
            'Education': 1.5,
            'Technology': 1.4,
            'Finance': 2.0,
            'Gaming': 0.8,
            'Entertainment': 0.7,
            'Music': 0.6
        }
        
        # ì›” ìˆ˜ìµ ì˜ˆì¸¡ (ì£¼ 2íšŒ ì—…ë¡œë“œ ê°€ì •)
        monthly_videos = 8
        monthly_views = avg_views * monthly_videos
        monthly_ad_revenue = (monthly_views / 1000) * base_cpm
        
        # ìŠ¤í°ì„œì‹­ ìˆ˜ìµ ì˜ˆì¸¡
        if subscribers >= 100000:
            sponsorship_rate = subscribers * 100  # êµ¬ë…ìë‹¹ 100ì›
            sponsorship_tier = "Aê¸‰ (ëŒ€í˜• ë¸Œëœë“œ)"
        elif subscribers >= 50000:
            sponsorship_rate = subscribers * 70
            sponsorship_tier = "Bê¸‰ (ì¤‘ê²¬ ë¸Œëœë“œ)"  
        elif subscribers >= 10000:
            sponsorship_rate = subscribers * 40
            sponsorship_tier = "Cê¸‰ (ì†Œê·œëª¨ ë¸Œëœë“œ)"
        else:
            sponsorship_rate = subscribers * 15
            sponsorship_tier = "Dê¸‰ (ë¡œì»¬ ë¸Œëœë“œ)"
        
        # ê¸°íƒ€ ìˆ˜ìµì›
        membership_revenue = subscribers * 0.01 * 4900  # êµ¬ë…ìì˜ 1%ê°€ ë©¤ë²„ì‹­ ê°€ì…
        superchat_revenue = monthly_views * 0.001 * 500  # ì¡°íšŒìˆ˜ì˜ 0.1%ê°€ ìŠˆí¼ì±—
        
        # ì´ ì˜ˆìƒ ìˆ˜ìµ
        total_monthly_revenue = monthly_ad_revenue + (sponsorship_rate / 12) + membership_revenue + superchat_revenue
        
        # ROI ì§€í‘œ
        metrics = {
            'monthly_revenue_estimate': {
                'advertising': int(monthly_ad_revenue),
                'sponsorship_monthly': int(sponsorship_rate / 12),
                'membership': int(membership_revenue),
                'superchat': int(superchat_revenue),
                'total': int(total_monthly_revenue)
            },
            'sponsorship_info': {
                'per_video_rate': int(sponsorship_rate),
                'tier': sponsorship_tier,
                'annual_potential': int(sponsorship_rate * 6)  # ë…„ 6ê±´ ê°€ì •
            },
            'growth_metrics': {
                'views_per_subscriber': avg_views / subscribers if subscribers > 0 else 0,
                'engagement_quality_score': min(100, avg_engagement * 20),
                'content_consistency': content_stats.get('upload_consistency_score', 0),
                'monetization_readiness': self._calculate_monetization_readiness(subscribers, total_views, avg_engagement)
            },
            'market_position': {
                'subscriber_tier': self._get_subscriber_tier(subscribers),
                'growth_stage': self._determine_growth_stage(subscribers, avg_views),
                'competition_level': self._assess_competition_level(channel_data, content_stats)
            }
        }
        
        return metrics
    
    def _calculate_monetization_readiness(self, subscribers: int, total_views: int, engagement: float) -> int:
        """ìˆ˜ìµí™” ì¤€ë¹„ë„ ì ìˆ˜ (0-100)"""
        score = 0
        
        # êµ¬ë…ì ì ìˆ˜ (40ì )
        if subscribers >= 100000:
            score += 40
        elif subscribers >= 10000:
            score += 30
        elif subscribers >= 1000:
            score += 20
        elif subscribers >= 100:
            score += 10
        
        # ì¡°íšŒìˆ˜ ì ìˆ˜ (30ì )
        if total_views >= 10000000:
            score += 30
        elif total_views >= 1000000:
            score += 20
        elif total_views >= 100000:
            score += 15
        elif total_views >= 10000:
            score += 10
        
        # ì°¸ì—¬ìœ¨ ì ìˆ˜ (30ì )
        if engagement >= 10:
            score += 30
        elif engagement >= 5:
            score += 20
        elif engagement >= 2:
            score += 10
        elif engagement >= 1:
            score += 5
        
        return min(100, score)
    
    def _get_subscriber_tier(self, subscribers: int) -> str:
        """êµ¬ë…ì ë“±ê¸‰ ë¶„ë¥˜"""
        if subscribers >= 1000000:
            return "ë©”ê°€ ì¸í”Œë£¨ì–¸ì„œ (100ë§Œ+)"
        elif subscribers >= 100000:
            return "ë§¤í¬ë¡œ ì¸í”Œë£¨ì–¸ì„œ (10-100ë§Œ)"
        elif subscribers >= 10000:
            return "ë§ˆì´í¬ë¡œ ì¸í”Œë£¨ì–¸ì„œ (1-10ë§Œ)"
        elif subscribers >= 1000:
            return "ë‚˜ë…¸ ì¸í”Œë£¨ì–¸ì„œ (1ì²œ-1ë§Œ)"
        else:
            return "ì‹ ê·œ í¬ë¦¬ì—ì´í„° (1ì²œ ë¯¸ë§Œ)"
    
    def _determine_growth_stage(self, subscribers: int, avg_views: int) -> str:
        """ì„±ì¥ ë‹¨ê³„ íŒë‹¨"""
        view_to_sub_ratio = avg_views / subscribers if subscribers > 0 else 0
        
        if view_to_sub_ratio >= 0.5:
            return "ê¸‰ì„±ì¥ ë‹¨ê³„"
        elif view_to_sub_ratio >= 0.2:
            return "ì„±ì¥ ë‹¨ê³„"  
        elif view_to_sub_ratio >= 0.1:
            return "ì•ˆì • ë‹¨ê³„"
        else:
            return "ì •ì²´ ë‹¨ê³„"
    
    def _assess_competition_level(self, channel_data: Dict, content_stats: Dict) -> str:
        """ê²½ìŸ ê°•ë„ í‰ê°€"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
        avg_views = content_stats.get('avg_views', 0)
        subscribers = channel_data.get('subscriber_count', 0)
        
        if avg_views > subscribers * 0.3:
            return "ì €ê²½ìŸ (ë¸”ë£¨ì˜¤ì…˜)"
        elif avg_views > subscribers * 0.1:
            return "ì¤‘ê°„ê²½ìŸ (ì„±ì¥ ê°€ëŠ¥)"
        else:
            return "ê³ ê²½ìŸ (ë ˆë“œì˜¤ì…˜)"
    
    def generate_comprehensive_report(self, channel_data: Dict, videos: List[Dict]) -> str:
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ê°ì¢… ë¶„ì„ ì‹¤í–‰
        content_stats = self.analyze_content_performance(videos)
        title_analysis = self.analyze_title_patterns(videos)
        upload_patterns = self.analyze_upload_patterns(videos)
        topic_analysis = self.analyze_content_topics(videos)
        business_metrics = self.calculate_business_metrics(channel_data, content_stats)
        
        report = f"""
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                    YouTube ì±„ë„ ì¢…í•© ì‚¬ì—…ì„± ë¶„ì„ ë³´ê³ ì„œ
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

ğŸ“º ì±„ë„ ê¸°ë³¸ ì •ë³´
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ì±„ë„ëª…: {channel_data.get('channel_name', 'Unknown')}
êµ¬ë…ì: {channel_data.get('subscriber_count', 0):,}ëª…
ì´ ì¡°íšŒìˆ˜: {channel_data.get('total_views', 0):,}íšŒ
ì´ ë™ì˜ìƒ: {channel_data.get('video_count', 0)}ê°œ
ê°œì„¤ì¼: {channel_data.get('created_date', 'Unknown')[:10]}

ğŸ“Š ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ë¶„ì„ ëŒ€ìƒ ì˜ìƒ: {content_stats.get('total_videos', 0)}ê°œ
í‰ê·  ì¡°íšŒìˆ˜: {content_stats.get('avg_views', 0):,.0f}íšŒ
ìµœê³  ì¡°íšŒìˆ˜: {content_stats.get('max_views', 0):,}íšŒ
í‰ê·  ì°¸ì—¬ìœ¨: {content_stats.get('avg_engagement_rate', 0):.2f}%
í‰ê·  ì˜ìƒ ê¸¸ì´: {content_stats.get('avg_duration', 0):.1f}ë¶„
ì´ ì‹œì²­ ì‹œê°„: {content_stats.get('total_watch_time', 0):,.0f}ì‹œê°„

ğŸ¯ ì½˜í…ì¸  ì£¼ì œ ë¶„ì„  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ì£¼ìš” ì½˜í…ì¸  ìœ í˜•: {topic_analysis.get('most_common_topic', 'Unknown')}
ìµœê³  ì„±ê³¼ ì£¼ì œ: {topic_analysis.get('best_performing_topic', 'Unknown')}

ì£¼ì œë³„ ë¶„í¬:"""
        
        # ì£¼ì œ ë¶„í¬ ì¶”ê°€
        topic_dist = topic_analysis.get('topic_distribution', {})
        for topic, score in sorted(topic_dist.items(), key=lambda x: x[1], reverse=True)[:5]:
            if score > 0:
                report += f"\n  â€¢ {topic}: {score}ê°œ í‚¤ì›Œë“œ"
        
        report += f"""

ğŸ“ ì œëª© íŒ¨í„´ ë¶„ì„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
í‰ê·  ì œëª© ê¸¸ì´: {title_analysis.get('avg_title_length', 0):.0f}ì
ê¸ì •ì  ì œëª© ë¹„ìœ¨: {title_analysis.get('positive_titles_ratio', 0)*100:.1f}%
ìˆ«ì í¬í•¨ ì œëª©: {title_analysis.get('special_patterns', {}).get('numbers_in_titles', 0)}ê°œ

ì¸ê¸° í‚¤ì›Œë“œ TOP 5:"""
        
        # ì¸ê¸° í‚¤ì›Œë“œ ì¶”ê°€
        top_words = title_analysis.get('top_korean_words', [])[:5]
        for i, (word, count) in enumerate(top_words, 1):
            report += f"\n  {i}. {word} ({count}íšŒ)"
        
        report += f"""

â° ì—…ë¡œë“œ íŒ¨í„´ ë¶„ì„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ì—…ë¡œë“œ ë¹ˆë„: ì¼ {upload_patterns.get('videos_per_day', 0):.2f}ê°œ
ì—…ë¡œë“œ ì¼ê´€ì„±: {upload_patterns.get('upload_consistency_score', 0):.1f}/100ì 

ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€:"""
        
        # ì‹œê°„ëŒ€ë³„ ì„±ê³¼ëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ í‘œì‹œ
        upload_dist = upload_patterns.get('upload_distribution', {}).get('hourly', {})
        if upload_dist:
            best_hours = sorted(upload_dist.items(), key=lambda x: x[1], reverse=True)[:3]
            for hour, count in best_hours:
                report += f"\n  â€¢ {hour}ì‹œ: {count}ê°œ ì—…ë¡œë“œ"
        
        report += f"""

ğŸ’° ì˜ˆìƒ ìˆ˜ìµ ë¶„ì„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ì›” ì˜ˆìƒ ì´ ìˆ˜ìµ: {business_metrics['monthly_revenue_estimate']['total']:,}ì›

ìˆ˜ìµ êµ¬ì„±:
  â€¢ ê´‘ê³  ìˆ˜ìµ: {business_metrics['monthly_revenue_estimate']['advertising']:,}ì›
  â€¢ ìŠ¤í°ì„œì‹­: {business_metrics['monthly_revenue_estimate']['sponsorship_monthly']:,}ì›/ì›”
  â€¢ ë©¤ë²„ì‹­: {business_metrics['monthly_revenue_estimate']['membership']:,}ì›
  â€¢ ìŠˆí¼ì±—: {business_metrics['monthly_revenue_estimate']['superchat']:,}ì›

ìŠ¤í°ì„œì‹­ ì •ë³´:
  â€¢ ì˜ìƒë‹¹ ë‹¨ê°€: {business_metrics['sponsorship_info']['per_video_rate']:,}ì›
  â€¢ ë¸Œëœë“œ ë“±ê¸‰: {business_metrics['sponsorship_info']['tier']}
  â€¢ ì—° ì˜ˆìƒ ìˆ˜ìµ: {business_metrics['sponsorship_info']['annual_potential']:,}ì›

ğŸ“ˆ ì„±ì¥ ì§€í‘œ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
êµ¬ë…ì ë“±ê¸‰: {business_metrics['market_position']['subscriber_tier']}
ì„±ì¥ ë‹¨ê³„: {business_metrics['market_position']['growth_stage']}
ì‹œì¥ ê²½ìŸë„: {business_metrics['market_position']['competition_level']}
ìˆ˜ìµí™” ì¤€ë¹„ë„: {business_metrics['growth_metrics']['monetization_readiness']}/100ì 
ì½˜í…ì¸  ì¼ê´€ì„±: {business_metrics['growth_metrics']['content_consistency']:.1f}/100ì 

ğŸ¬ ì½˜í…ì¸  ì„±ê³¼ ë“±ê¸‰ë³„ ë¶„ì„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
        
        # ì„±ê³¼ ë“±ê¸‰ë³„ ë¶„ì„
        tiers = content_stats.get('performance_tiers', {})
        for tier, data in tiers.items():
            report += f"""
ã€{tier} ì„±ê³¼ ì˜ìƒã€‘
  â€¢ ê°œìˆ˜: {data['count']}ê°œ
  â€¢ í‰ê·  ì¡°íšŒìˆ˜: {data['avg_views']:,.0f}íšŒ
  â€¢ í‰ê·  ì°¸ì—¬ìœ¨: {data['avg_engagement']:.2f}%
  â€¢ í‰ê·  ê¸¸ì´: {data['avg_duration']:.1f}ë¶„"""
        
        report += f"""

ğŸ’¡ ì‚¬ì—… ê¸°íšŒ ë° ì¶”ì²œì‚¬í•­
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = []
        
        # êµ¬ë…ì ê¸°ì¤€ ì¶”ì²œ
        subs = channel_data.get('subscriber_count', 0)
        if subs < 1000:
            recommendations.append("ğŸ¯ ìš°ì„ ìˆœìœ„: 1,000ëª… êµ¬ë…ì ë‹¬ì„± (ìˆ˜ìµí™” ì¡°ê±´)")
            recommendations.append("  - ì‡¼ì¸  ì½˜í…ì¸  í™œìš©í•˜ì—¬ ë…¸ì¶œ ê·¹ëŒ€í™”")
            recommendations.append("  - íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì ê·¹ í™œìš©")
        elif subs < 10000:
            recommendations.append("ğŸ¯ ìš°ì„ ìˆœìœ„: 1ë§Œëª… ëŒíŒŒ (ë§ˆì´í¬ë¡œ ì¸í”Œë£¨ì–¸ì„œ)")
            recommendations.append("  - ìŠ¤í°ì„œì‹­ ê¸°íšŒ íƒìƒ‰ ì‹œì‘")
            recommendations.append("  - ì»¤ë®¤ë‹ˆí‹° êµ¬ì¶• ë° ì°¸ì—¬ìœ¨ í–¥ìƒ")
        else:
            recommendations.append("ğŸ¯ ìš°ì„ ìˆœìœ„: ìˆ˜ìµ ë‹¤ê°í™” ë° ë¸Œëœë“œ êµ¬ì¶•")
            recommendations.append("  - í”„ë¦¬ë¯¸ì—„ ìŠ¤í°ì„œì‹­ ì ê·¹ ìœ ì¹˜")
            recommendations.append("  - ìì²´ ìƒí’ˆ/ì„œë¹„ìŠ¤ ê°œë°œ ê²€í† ")
        
        # ì°¸ì—¬ìœ¨ ê¸°ì¤€ ì¶”ì²œ
        engagement = content_stats.get('avg_engagement_rate', 0)
        if engagement < 2:
            recommendations.append("ğŸ“¢ ì°¸ì—¬ìœ¨ ê°œì„  í•„ìš”")
            recommendations.append("  - CTA ê°•í™” ë° ì»¤ë®¤ë‹ˆí‹° ì†Œí†µ ì¦ëŒ€")
        elif engagement > 5:
            recommendations.append("â­ ë†’ì€ ì°¸ì—¬ìœ¨ í™œìš©")
            recommendations.append("  - ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ë° ì‹¤ì‹œê°„ ì†Œí†µ í™•ëŒ€")
        
        # ì½˜í…ì¸  ì „ëµ ì¶”ì²œ
        best_topic = topic_analysis.get('best_performing_topic')
        if best_topic:
            recommendations.append(f"ğŸ¬ '{best_topic}' ì½˜í…ì¸  í™•ëŒ€ ê¶Œì¥")
        
        for rec in recommendations:
            report += f"\n{rec}"
        
        report += f"""

âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ë¦¬ìŠ¤í¬
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ ìˆ˜ìµ ì˜ˆì¸¡ì€ í‰ê· ê°’ ê¸°ì¤€ì´ë©° ì‹¤ì œì™€ ì°¨ì´ ìˆì„ ìˆ˜ ìˆìŒ
â€¢ YouTube ì •ì±… ë³€í™”ì— ë”°ë¥¸ ìˆ˜ìµ ë³€ë™ ê°€ëŠ¥ì„±
â€¢ ê²½ìŸ ì‹¬í™”ë¡œ ì¸í•œ ì„±ì¥ë¥  ë‘”í™” ë¦¬ìŠ¤í¬
â€¢ ì½˜í…ì¸  íŠ¸ë Œë“œ ë³€í™” ëŒ€ì‘ í•„ìš”ì„±

ğŸ“Š ë°ì´í„° ê¸°ì¤€
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ë¶„ì„ì¼: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}
ë¶„ì„ ì˜ìƒ ìˆ˜: {len(videos)}ê°œ
ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•: YouTube API/ì›¹ í¬ë¡¤ë§

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                              ë³´ê³ ì„œ ë
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
"""
        
        return report
    
    def save_analysis_results(self, channel_name: str, report: str, data: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
        filename = f"business_analysis_{channel_name.replace(' ', '_')}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # JSON ë°ì´í„° ì €ì¥
        json_filename = f"analysis_data_{channel_name.replace(' ', '_')}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ë¶„ì„ ì™„ë£Œ: {filename}, {json_filename} ì €ì¥ë¨")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    analyzer = YouTubeContentAnalyzer()
    
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_channel = {
        'channel_name': 'ë¡œì§ì•Œë ¤ì£¼ëŠ”ë‚¨ì',
        'subscriber_count': 50000,
        'total_views': 5000000,
        'video_count': 200,
        'created_date': '2022-01-01'
    }
    
    sample_videos = [
        {
            'title': 'íŒŒì´ì¬ ê¸°ì´ˆ ê°•ì˜ 1í¸',
            'view_count': 25000,
            'like_count': 800,
            'comment_count': 150,
            'duration': 600,
            'published_date': '2024-01-15T10:00:00Z',
            'description': 'íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆë¥¼ ë°°ì›Œë³´ì„¸ìš”'
        }
        # ë” ë§ì€ ìƒ˜í”Œ ë°ì´í„°...
    ]
    
    report = analyzer.generate_comprehensive_report(sample_channel, sample_videos)
    print(report)