import requests
from bs4 import BeautifulSoup
import json
import re
from typing import Dict, List
from datetime import datetime
import pandas as pd
from googleapiclient.discovery import build
import os

class YouTubeChannelScraper:
    """YouTube ì±„ë„ ì½˜í…ì¸  ìˆ˜ì§‘ ë° ë¶„ì„ ë„êµ¬"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key
        if api_key:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
        else:
            self.youtube = None
            
    def extract_channel_id(self, channel_url: str) -> str:
        """ì±„ë„ URLì—ì„œ ì±„ë„ ID ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ YouTube URL í˜•ì‹ ì²˜ë¦¬
        patterns = [
            r'youtube\.com/channel/([a-zA-Z0-9_-]+)',
            r'youtube\.com/c/([a-zA-Z0-9_-]+)',
            r'youtube\.com/@([a-zA-Z0-9_-]+)',
            r'youtube\.com/user/([a-zA-Z0-9_-]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, channel_url)
            if match:
                return match.group(1)
        
        return None
    
    def get_channel_info_via_api(self, channel_id: str) -> Dict:
        """YouTube APIë¥¼ í†µí•œ ì±„ë„ ì •ë³´ ìˆ˜ì§‘"""
        if not self.youtube:
            return self.get_channel_info_via_scraping(channel_id)
        
        try:
            # ì±„ë„ ê¸°ë³¸ ì •ë³´
            channel_response = self.youtube.channels().list(
                part='snippet,statistics,contentDetails',
                id=channel_id
            ).execute()
            
            if not channel_response['items']:
                return None
            
            channel_data = channel_response['items'][0]
            
            # ìµœê·¼ ë™ì˜ìƒ ëª©ë¡
            videos = self.get_recent_videos(channel_id, max_results=50)
            
            return {
                'channel_id': channel_id,
                'channel_name': channel_data['snippet']['title'],
                'description': channel_data['snippet']['description'],
                'subscriber_count': int(channel_data['statistics'].get('subscriberCount', 0)),
                'total_views': int(channel_data['statistics'].get('viewCount', 0)),
                'video_count': int(channel_data['statistics'].get('videoCount', 0)),
                'created_date': channel_data['snippet']['publishedAt'],
                'country': channel_data['snippet'].get('country', 'Unknown'),
                'recent_videos': videos
            }
            
        except Exception as e:
            print(f"API ì˜¤ë¥˜: {e}")
            return self.get_channel_info_via_scraping(channel_id)
    
    def get_channel_info_via_scraping(self, channel_url: str) -> Dict:
        """ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•œ ì±„ë„ ì •ë³´ ìˆ˜ì§‘ (API í‚¤ ì—†ì„ ë•Œ)"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        try:
            response = requests.get(channel_url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # YouTube í˜ì´ì§€ì˜ JSON ë°ì´í„° ì¶”ì¶œ
            scripts = soup.find_all('script')
            channel_data = {}
            
            for script in scripts:
                if 'var ytInitialData' in script.text:
                    json_text = script.text.split('var ytInitialData = ')[1].split(';</script>')[0]
                    data = json.loads(json_text)
                    
                    # ì±„ë„ ì •ë³´ íŒŒì‹±
                    header = data.get('header', {}).get('c4TabbedHeaderRenderer', {})
                    if header:
                        channel_data = {
                            'channel_name': header.get('title', ''),
                            'subscriber_count': self.parse_subscriber_count(header.get('subscriberCountText', {}).get('simpleText', '0')),
                            'video_count': self.extract_video_count(data),
                            'description': self.extract_description(data)
                        }
                    break
            
            return channel_data
            
        except Exception as e:
            print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def get_recent_videos(self, channel_id: str, max_results: int = 50) -> List[Dict]:
        """ìµœê·¼ ì—…ë¡œë“œëœ ë™ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not self.youtube:
            return []
        
        try:
            # ì±„ë„ì˜ ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ID ê°€ì ¸ì˜¤ê¸°
            channel_response = self.youtube.channels().list(
                part='contentDetails',
                id=channel_id
            ).execute()
            
            uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
            
            # ì¬ìƒëª©ë¡ì—ì„œ ë™ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
            videos = []
            next_page_token = None
            
            while len(videos) < max_results:
                playlist_response = self.youtube.playlistItems().list(
                    part='snippet',
                    playlistId=uploads_playlist_id,
                    maxResults=min(50, max_results - len(videos)),
                    pageToken=next_page_token
                ).execute()
                
                for item in playlist_response['items']:
                    video_id = item['snippet']['resourceId']['videoId']
                    video_details = self.get_video_details(video_id)
                    videos.append(video_details)
                
                next_page_token = playlist_response.get('nextPageToken')
                if not next_page_token:
                    break
            
            return videos
            
        except Exception as e:
            print(f"ë™ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def get_video_details(self, video_id: str) -> Dict:
        """ê°œë³„ ë™ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if not self.youtube:
            return {}
        
        try:
            video_response = self.youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=video_id
            ).execute()
            
            if not video_response['items']:
                return {}
            
            video = video_response['items'][0]
            
            return {
                'video_id': video_id,
                'title': video['snippet']['title'],
                'description': video['snippet']['description'][:500],
                'published_date': video['snippet']['publishedAt'],
                'duration': self.parse_duration(video['contentDetails']['duration']),
                'view_count': int(video['statistics'].get('viewCount', 0)),
                'like_count': int(video['statistics'].get('likeCount', 0)),
                'comment_count': int(video['statistics'].get('commentCount', 0)),
                'tags': video['snippet'].get('tags', [])[:10],
                'category_id': video['snippet'].get('categoryId', ''),
                'thumbnail_url': video['snippet']['thumbnails']['high']['url']
            }
            
        except Exception as e:
            print(f"ë™ì˜ìƒ ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {e}")
            return {}
    
    def parse_duration(self, duration: str) -> int:
        """ISO 8601 ê¸°ê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        import isodate
        try:
            return int(isodate.parse_duration(duration).total_seconds())
        except:
            return 0
    
    def parse_subscriber_count(self, text: str) -> int:
        """êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜"""
        text = text.replace('subscribers', '').replace('êµ¬ë…ì', '').strip()
        
        multipliers = {
            'K': 1000,
            'M': 1000000,
            'ì²œ': 1000,
            'ë§Œ': 10000,
            'ì–µ': 100000000
        }
        
        for suffix, multiplier in multipliers.items():
            if suffix in text:
                number = float(re.findall(r'[\d.]+', text)[0])
                return int(number * multiplier)
        
        # ì¼ë°˜ ìˆ«ì
        numbers = re.findall(r'\d+', text.replace(',', ''))
        return int(numbers[0]) if numbers else 0
    
    def extract_video_count(self, data: Dict) -> int:
        """YouTube ë°ì´í„°ì—ì„œ ë™ì˜ìƒ ìˆ˜ ì¶”ì¶œ"""
        try:
            tabs = data.get('contents', {}).get('twoColumnBrowseResultsRenderer', {}).get('tabs', [])
            for tab in tabs:
                if 'tabRenderer' in tab:
                    if 'Videos' in tab['tabRenderer'].get('title', ''):
                        text = tab['tabRenderer'].get('content', {}).get('sectionListRenderer', {})
                        # ë™ì˜ìƒ ìˆ˜ ì¶”ì¶œ ë¡œì§
                        return 0  # ì‹¤ì œ êµ¬í˜„ í•„ìš”
        except:
            pass
        return 0
    
    def extract_description(self, data: Dict) -> str:
        """ì±„ë„ ì„¤ëª… ì¶”ì¶œ"""
        try:
            about_data = data.get('metadata', {}).get('channelMetadataRenderer', {})
            return about_data.get('description', '')
        except:
            return ''
    
    def analyze_content_pattern(self, videos: List[Dict]) -> Dict:
        """ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„"""
        if not videos:
            return {}
        
        df = pd.DataFrame(videos)
        
        # ì—…ë¡œë“œ íŒ¨í„´
        df['published_date'] = pd.to_datetime(df['published_date'])
        df['day_of_week'] = df['published_date'].dt.day_name()
        df['hour'] = df['published_date'].dt.hour
        
        # ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„
        avg_views = df['view_count'].mean()
        avg_likes = df['like_count'].mean()
        avg_comments = df['comment_count'].mean()
        
        # ì¸ê¸° ë™ì˜ìƒ (ìƒìœ„ 10%)
        top_10_percent = int(len(df) * 0.1) or 1
        top_videos = df.nlargest(top_10_percent, 'view_count')
        
        # ì œëª© í‚¤ì›Œë“œ ë¶„ì„
        all_titles = ' '.join(df['title'].tolist())
        common_words = self.extract_common_words(all_titles)
        
        # ìµœì  ì—…ë¡œë“œ ì‹œê°„
        best_time = df.groupby('hour')['view_count'].mean().idxmax()
        best_day = df.groupby('day_of_week')['view_count'].mean().idxmax()
        
        # ë™ì˜ìƒ ê¸¸ì´ ë¶„ì„
        df['duration_minutes'] = df['duration'] / 60
        optimal_duration = df.loc[df['view_count'] > avg_views, 'duration_minutes'].median()
        
        return {
            'total_videos_analyzed': len(videos),
            'average_views': int(avg_views),
            'average_likes': int(avg_likes),
            'average_comments': int(avg_comments),
            'engagement_rate': (avg_likes + avg_comments) / avg_views * 100 if avg_views > 0 else 0,
            'top_videos': top_videos[['title', 'view_count', 'like_count']].to_dict('records'),
            'common_keywords': common_words[:10],
            'best_upload_time': f"{best_day} {best_time}:00",
            'optimal_video_duration': f"{optimal_duration:.1f} minutes",
            'upload_frequency': self.calculate_upload_frequency(df),
            'growth_trend': self.analyze_growth_trend(df)
        }
    
    def extract_common_words(self, text: str, min_length: int = 3) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ"""
        import collections
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'as', 'are', 
                     'was', 'were', 'of', 'to', 'in', 'for', 'with', 'ì˜', 'ë¥¼', 'ì„', 'ì´', 'ê°€'}
        
        words = re.findall(r'\b\w+\b', text.lower())
        words = [w for w in words if len(w) >= min_length and w not in stop_words]
        
        word_counts = collections.Counter(words)
        return [word for word, count in word_counts.most_common(20)]
    
    def calculate_upload_frequency(self, df: pd.DataFrame) -> str:
        """ì—…ë¡œë“œ ë¹ˆë„ ê³„ì‚°"""
        date_diff = (df['published_date'].max() - df['published_date'].min()).days
        if date_diff == 0:
            return "ì¼ì¼ ì—…ë¡œë“œ"
        
        videos_per_day = len(df) / date_diff
        
        if videos_per_day >= 1:
            return f"ì¼ {videos_per_day:.1f}ê°œ"
        elif videos_per_day >= 0.3:
            return f"ì£¼ {videos_per_day * 7:.1f}ê°œ"
        else:
            return f"ì›” {videos_per_day * 30:.1f}ê°œ"
    
    def analyze_growth_trend(self, df: pd.DataFrame) -> str:
        """ì„±ì¥ íŠ¸ë Œë“œ ë¶„ì„"""
        # ìµœê·¼ 10ê°œ vs ì´ì „ 10ê°œ ë¹„êµ
        if len(df) < 20:
            return "ë°ì´í„° ë¶€ì¡±"
        
        recent = df.head(10)['view_count'].mean()
        older = df.iloc[10:20]['view_count'].mean()
        
        if older == 0:
            return "ê¸‰ì„±ì¥"
        
        growth_rate = ((recent - older) / older) * 100
        
        if growth_rate > 50:
            return f"ê¸‰ì„±ì¥ (â†‘{growth_rate:.0f}%)"
        elif growth_rate > 20:
            return f"ì„±ì¥ì¤‘ (â†‘{growth_rate:.0f}%)"
        elif growth_rate > -20:
            return "ì•ˆì •ì "
        else:
            return f"í•˜ë½ì¤‘ (â†“{abs(growth_rate):.0f}%)"
    
    def generate_business_analysis(self, channel_data: Dict, content_analysis: Dict) -> str:
        """ì‚¬ì—…ì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
================================================================================
                        YouTube ì±„ë„ ì‚¬ì—…ì„± ë¶„ì„ ë³´ê³ ì„œ
================================================================================

ğŸ“º ì±„ë„ ì •ë³´
--------------------------------------------------------------------------------
ì±„ë„ëª…: {channel_data.get('channel_name', 'Unknown')}
êµ¬ë…ì: {channel_data.get('subscriber_count', 0):,}ëª…
ì´ ì¡°íšŒìˆ˜: {channel_data.get('total_views', 0):,}íšŒ
ë™ì˜ìƒ ìˆ˜: {channel_data.get('video_count', 0):,}ê°œ
ì±„ë„ ìƒì„±ì¼: {channel_data.get('created_date', 'Unknown')[:10]}

ğŸ“Š ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„
--------------------------------------------------------------------------------
í‰ê·  ì¡°íšŒìˆ˜: {content_analysis.get('average_views', 0):,}íšŒ
í‰ê·  ì¢‹ì•„ìš”: {content_analysis.get('average_likes', 0):,}ê°œ
í‰ê·  ëŒ“ê¸€: {content_analysis.get('average_comments', 0):,}ê°œ
ì°¸ì—¬ìœ¨: {content_analysis.get('engagement_rate', 0):.2f}%
ì—…ë¡œë“œ ë¹ˆë„: {content_analysis.get('upload_frequency', 'Unknown')}
ì„±ì¥ íŠ¸ë Œë“œ: {content_analysis.get('growth_trend', 'Unknown')}

ğŸ¯ ì½˜í…ì¸  ì „ëµ ë¶„ì„
--------------------------------------------------------------------------------
ìµœì  ì—…ë¡œë“œ ì‹œê°„: {content_analysis.get('best_upload_time', 'Unknown')}
ìµœì  ë™ì˜ìƒ ê¸¸ì´: {content_analysis.get('optimal_video_duration', 'Unknown')}
ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(content_analysis.get('common_keywords', [])[:5])}

ğŸ’° ì˜ˆìƒ ìˆ˜ìµ ë¶„ì„
--------------------------------------------------------------------------------
"""
        
        # ìˆ˜ìµ ê³„ì‚°
        monthly_views = content_analysis.get('average_views', 0) * 8  # ì›” 8ê°œ ì˜ìƒ ê°€ì •
        cpm = 2000  # í‰ê·  CPM (ì›)
        monthly_ad_revenue = (monthly_views / 1000) * cpm
        
        # ìŠ¤í°ì„œì‹­ ê°€ëŠ¥ì„±
        subscribers = channel_data.get('subscriber_count', 0)
        if subscribers >= 100000:
            sponsorship = subscribers * 50  # êµ¬ë…ìë‹¹ 50ì›
            sponsorship_tier = "í”„ë¦¬ë¯¸ì—„"
        elif subscribers >= 10000:
            sponsorship = subscribers * 30
            sponsorship_tier = "ì¤‘ê¸‰"
        else:
            sponsorship = subscribers * 10
            sponsorship_tier = "ì´ˆê¸‰"
        
        report += f"""ì›” ì˜ˆìƒ ê´‘ê³  ìˆ˜ìµ: {int(monthly_ad_revenue):,}ì›
ì›” ì˜ˆìƒ ìŠ¤í°ì„œì‹­ (1ê±´): {int(sponsorship):,}ì› ({sponsorship_tier})
ì´ ì›” ì˜ˆìƒ ìˆ˜ìµ: {int(monthly_ad_revenue + sponsorship):,}ì›

ğŸ† ì¸ê¸° ì½˜í…ì¸  TOP 3
--------------------------------------------------------------------------------"""
        
        top_videos = content_analysis.get('top_videos', [])[:3]
        for i, video in enumerate(top_videos, 1):
            report += f"""
{i}. {video['title']}
   ì¡°íšŒìˆ˜: {video['view_count']:,} | ì¢‹ì•„ìš”: {video['like_count']:,}"""
        
        # ì‚¬ì—… ê¸°íšŒ í‰ê°€
        report += """

ğŸ“ˆ ì‚¬ì—… ê¸°íšŒ í‰ê°€
--------------------------------------------------------------------------------"""
        
        opportunities = []
        risks = []
        
        # ê¸°íšŒ ë¶„ì„
        if content_analysis.get('engagement_rate', 0) > 5:
            opportunities.append("ë†’ì€ ì°¸ì—¬ìœ¨ - ì¶©ì„± ê³ ê°ì¸µ ë³´ìœ ")
        if 'growth' in content_analysis.get('growth_trend', '').lower():
            opportunities.append("ì„±ì¥ ì¶”ì„¸ - í™•ì¥ ê°€ëŠ¥ì„± ë†’ìŒ")
        if subscribers > 10000:
            opportunities.append("ìˆ˜ìµí™” ê°€ëŠ¥ - ë‹¤ì–‘í•œ ìˆ˜ìµì› í™œìš© ê°€ëŠ¥")
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        if content_analysis.get('engagement_rate', 0) < 2:
            risks.append("ë‚®ì€ ì°¸ì—¬ìœ¨ - ì½˜í…ì¸  ê°œì„  í•„ìš”")
        if 'í•˜ë½' in content_analysis.get('growth_trend', ''):
            risks.append("í•˜ë½ ì¶”ì„¸ - ì „ëµ ì¬ê²€í†  í•„ìš”")
        if subscribers < 1000:
            risks.append("ìˆ˜ìµí™” ë¯¸ë‹¬ - ì„±ì¥ ì „ëµ í•„ìš”")
        
        report += f"""
âœ… ê¸°íšŒ ìš”ì¸:
{chr(10).join([f'  â€¢ {opp}' for opp in opportunities]) if opportunities else '  â€¢ ì¶”ê°€ ë¶„ì„ í•„ìš”'}

âš ï¸ ë¦¬ìŠ¤í¬ ìš”ì¸:
{chr(10).join([f'  â€¢ {risk}' for risk in risks]) if risks else '  â€¢ íŠ¹ë³„í•œ ë¦¬ìŠ¤í¬ ì—†ìŒ'}

ğŸ¬ ì¶”ì²œ ì „ëµ
--------------------------------------------------------------------------------"""
        
        # ì „ëµ ì¶”ì²œ
        if subscribers < 1000:
            report += """
1. êµ¬ë…ì í™•ë³´ ì§‘ì¤‘
   - ì‡¼ì¸  ì½˜í…ì¸  í™œìš©
   - í‚¤ì›Œë“œ ìµœì í™”
   - ì¸ë„¤ì¼ ê°œì„ 
"""
        elif subscribers < 10000:
            report += """
1. ì„±ì¥ ê°€ì†í™”
   - ì—…ë¡œë“œ ë¹ˆë„ ì¦ê°€
   - ì‹œë¦¬ì¦ˆë¬¼ ì œì‘
   - ì»¤ë®¤ë‹ˆí‹° í™œì„±í™”
"""
        else:
            report += """
1. ìˆ˜ìµ ë‹¤ê°í™”
   - ìŠ¤í°ì„œì‹­ ì ê·¹ ìœ ì¹˜
   - ë©¤ë²„ì‹­ ë„ì…
   - êµ¿ì¦ˆ/ìƒí’ˆ ê°œë°œ
"""
        
        report += """
================================================================================
"""
        
        return report

# ì‚¬ìš© ì˜ˆì œ
def analyze_channel(channel_url: str, api_key: str = None):
    """ì±„ë„ ë¶„ì„ ì‹¤í–‰"""
    scraper = YouTubeChannelScraper(api_key)
    
    print(f"ì±„ë„ ë¶„ì„ ì‹œì‘: {channel_url}")
    
    # ì±„ë„ ID ì¶”ì¶œ
    channel_id = scraper.extract_channel_id(channel_url)
    if not channel_id:
        print("ìœ íš¨í•œ ì±„ë„ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return
    
    # ì±„ë„ ì •ë³´ ìˆ˜ì§‘
    if api_key:
        channel_data = scraper.get_channel_info_via_api(channel_id)
    else:
        channel_data = scraper.get_channel_info_via_scraping(channel_url)
    
    if not channel_data:
        print("ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„
    videos = channel_data.get('recent_videos', [])
    content_analysis = scraper.analyze_content_pattern(videos) if videos else {}
    
    # ì‚¬ì—…ì„± ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    report = scraper.generate_business_analysis(channel_data, content_analysis)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    filename = f"channel_analysis_{channel_data.get('channel_name', 'unknown').replace(' ', '_')}.txt"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print(f"\në¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return channel_data, content_analysis

if __name__ == "__main__":
    # API í‚¤ê°€ ìˆëŠ” ê²½ìš° (ì„ íƒì‚¬í•­)
    # API_KEY = "YOUR_YOUTUBE_API_KEY"
    
    # API í‚¤ ì—†ì´ ë¶„ì„ (ì œí•œì )
    channel_url = "https://www.youtube.com/@ë¡œì§ì•Œë ¤ì£¼ëŠ”ë‚¨ì"
    
    # ë¶„ì„ ì‹¤í–‰
    analyze_channel(channel_url, api_key=None)